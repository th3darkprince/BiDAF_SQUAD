{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJzuqd7PYrYY"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "pLFKz22kYrYZ",
    "outputId": "113123a4-bbbe-4ea2-d28c-19d3e2a0e341"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                                                        #data import\n",
    "from nltk import word_tokenize                                             #word tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer                  #character tokenization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences          #padding\n",
    "from tqdm import tqdm                                                      #track progress\n",
    "import numpy as np                                                         #for numpy operations\n",
    "from sklearn.model_selection import train_test_split                       #to split the data into train and test sets\n",
    "import pickle                                                              #to save tokenizers\n",
    "from copy import deepcopy\n",
    "from numpy import save, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SB3glglYrYh"
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aje4HoNVYrY1",
    "outputId": "4aadb6db-8fa0-405b-c948-1bcfccda1421"
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"answerable_records.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "                                             context  \n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing records that do not have an answer\n",
    "df = df[df['answer'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB Tokenizer to tokenize the words and get start and end indices of the answer span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82532it [01:13, 1128.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#looping through records and getting unique words from the dataset using PTBTokenizer and mapping them to tokens\n",
    "word_dictionary = {}\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    \n",
    "    text = row['question'] + \" \" + row['context']\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    for j in tokens:\n",
    "        if j not in word_dictionary.keys():\n",
    "            word_dictionary[j] = len(word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110624\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in the total dataset(train + test): \",len(word_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "151DNimhIuco"
   },
   "outputs": [],
   "source": [
    "def get_start_end_words(row):\n",
    "    \"\"\"Function that loops through the context to find out the start and end indices \n",
    "    of the answer span using PTB tokenized words\"\"\"    \n",
    "    \n",
    "    answer = word_tokenize(row['answer'])\n",
    "    context = word_tokenize(row['context'])\n",
    "    \n",
    "    start_word=end_word=-1\n",
    "    \n",
    "    match=False\n",
    "   \n",
    "    for j in range(len(context)-len(answer)):\n",
    "        if context[j]==answer[0]:\n",
    "            match=True\n",
    "            k=0\n",
    "            for k in range(1, len(answer)):\n",
    "                if context[j+k]!=answer[k]:\n",
    "                    match=False\n",
    "            if match==True:\n",
    "                start_word=j\n",
    "                end_word=j+k\n",
    "                break\n",
    "  \n",
    "    row['start_word'] = start_word\n",
    "    row['end_word'] = end_word\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "YANAJ9y5Tqsk",
    "outputId": "7d283b86-5f3e-476f-bafe-b2defe28d14e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\squad\\lib\\site-packages\\tqdm\\std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 82532/82532 [02:42<00:00, 508.79it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "dataset =  df_answerable.progress_apply(get_start_end_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing records for which the function was not able to find the answer span(some more cleaning)\n",
    "dataset = dataset[dataset['start_word']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>start_word</th>\n",
       "      <th>end_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "                                             context  start_word  end_word  \n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...          50        53  \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...          38        40  \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...         104       104  \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...          30        32  \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...          52        53  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGDmjXPRYrY3"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NG4rBYoYrY3"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(dataset, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l0KaltUnYrY5",
    "outputId": "f0c52777-3d9a-48a7-853f-b7a6a40d260d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64769, 5) (16193, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sw2AmjCYrY6"
   },
   "source": [
    "## Word and Character Tokenization of Question and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cYRl66xRYrY7",
    "outputId": "4e8016ad-b587-4f67-92db-a878cb0ca99e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64769it [01:19, 809.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#building word tokenization dictionary of question and context\n",
    "word_tokenizer = {'PAD':0,'UNK':1}\n",
    "\n",
    "for i, row in tqdm(df_train.iterrows()):\n",
    "    \n",
    "    text = row['question'] + \" \" + row['context']\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    for j in tokens:\n",
    "        if j not in word_tokenizer.keys():\n",
    "            word_tokenizer[j] = len(word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZC38zGvoZV5"
   },
   "outputs": [],
   "source": [
    "#building character tokenization dictionary of question and context\n",
    "char_tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "char_tokenizer.fit_on_texts(df_train['question'] + df_train['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYZWiO8cbT-R"
   },
   "outputs": [],
   "source": [
    "def tokenize_entries(row):\n",
    "    \"\"\"Function that tokenizes(both word and character level) the question and context of all records\"\"\"\n",
    "    question = word_tokenize(row['question'])\n",
    "    context = word_tokenize(row['context'])\n",
    "    \n",
    "    question_word_tokens = []\n",
    "    context_word_tokens = []\n",
    "    question_char_tokens = []\n",
    "    context_char_tokens = []\n",
    "    \n",
    "    for i in question:\n",
    "        if i in word_tokenizer.keys():\n",
    "            question_word_tokens.append(word_tokenizer[i])\n",
    "            question_char_tokens.append(char_tokenizer.texts_to_sequences([i])[0])\n",
    "        else:\n",
    "            question_word_tokens.append(word_tokenizer['UNK'])\n",
    "            question_char_tokens.append(char_tokenizer.texts_to_sequences([i])[0])\n",
    "            \n",
    "    for i in context:\n",
    "        if i in word_tokenizer.keys():\n",
    "            context_word_tokens.append(word_tokenizer[i])\n",
    "            context_char_tokens.append(char_tokenizer.texts_to_sequences([i])[0])\n",
    "        else:\n",
    "            context_word_tokens.append(word_tokenizer['UNK'])\n",
    "            context_char_tokens.append(char_tokenizer.texts_to_sequences([i])[0])\n",
    "            \n",
    "    row['question_word_tokens'] = question_word_tokens\n",
    "    row['context_word_tokens'] = context_word_tokens\n",
    "    row['question_char_tokens'] = question_char_tokens\n",
    "    row['context_char_tokens'] = context_char_tokens\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLyblcNEf8WG"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2Ts6xFVUbT-T",
    "outputId": "4a33266b-88e9-423f-ed8e-bd81dd7f251c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64769/64769 [05:36<00:00, 192.35it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.progress_apply(tokenize_entries, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "trdCdQMcbT-U",
    "outputId": "b8409740-002d-4922-a93c-c37518fd40b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16193/16193 [01:20<00:00, 201.90it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.progress_apply(tokenize_entries, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "Lq7OkN-VE9X7",
    "outputId": "3e36adc6-c9bf-4517-d381-fd2767643895"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>start_word</th>\n",
       "      <th>end_word</th>\n",
       "      <th>question_word_tokens</th>\n",
       "      <th>context_word_tokens</th>\n",
       "      <th>question_char_tokens</th>\n",
       "      <th>context_char_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26862</th>\n",
       "      <td>What was replaced by the spinning wheel?</td>\n",
       "      <td>the traditional distaff</td>\n",
       "      <td>In agriculture, the increased usage of sheep w...</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>[10, 11, 12, 6, 13, 14, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>[[20, 11, 5, 4], [20, 5, 9], [10, 3, 18, 12, 5...</td>\n",
       "      <td>[[6, 7], [5, 19, 10, 6, 14, 15, 12, 4, 15, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91742</th>\n",
       "      <td>Around how many camps were set up by the Germa...</td>\n",
       "      <td>45 camps</td>\n",
       "      <td>Shortly after the end of the war in May 1945, ...</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>[102, 103, 104, 105, 62, 106, 107, 5, 6, 108, ...</td>\n",
       "      <td>[111, 112, 6, 113, 15, 6, 114, 82, 115, 116, 1...</td>\n",
       "      <td>[[5, 10, 8, 15, 7, 13], [11, 8, 20], [16, 5, 7...</td>\n",
       "      <td>[[9, 11, 8, 10, 4, 12, 21], [5, 17, 4, 3, 10],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15093</th>\n",
       "      <td>Who recognized hydrogen gas as a discreet subs...</td>\n",
       "      <td>Henry Cavendish</td>\n",
       "      <td>In 1671, Robert Boyle discovered and described...</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>[182, 183, 184, 185, 49, 21, 186, 187, 9]</td>\n",
       "      <td>[10, 188, 12, 189, 190, 191, 88, 192, 6, 193, ...</td>\n",
       "      <td>[[20, 11, 8], [10, 3, 14, 8, 19, 7, 6, 39, 3, ...</td>\n",
       "      <td>[[6, 7], [27, 46, 45, 27], [23], [10, 8, 22, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22826</th>\n",
       "      <td>On what date did the Japanese land on Enewetak?</td>\n",
       "      <td>September 29, 1914</td>\n",
       "      <td>In 1914, Japan joined the Entente during World...</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>[247, 248, 249, 250, 6, 251, 252, 59, 253, 9]</td>\n",
       "      <td>[10, 254, 12, 255, 256, 6, 257, 109, 258, 259,...</td>\n",
       "      <td>[[8, 7], [20, 11, 5, 4], [13, 5, 4, 3], [13, 6...</td>\n",
       "      <td>[[6, 7], [27, 33, 27, 44], [23], [34, 5, 18, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63456</th>\n",
       "      <td>How many \"voices\" did Montini's posters claim ...</td>\n",
       "      <td>1,000</td>\n",
       "      <td>During his period in Milan, Montini was known ...</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>[312, 104, 206, 313, 209, 250, 314, 246, 315, ...</td>\n",
       "      <td>[145, 321, 322, 82, 318, 12, 314, 3, 323, 49, ...</td>\n",
       "      <td>[[11, 8, 20], [16, 5, 7, 21], [471, 471], [24,...</td>\n",
       "      <td>[[13, 15, 10, 6, 7, 19], [11, 6, 9], [18, 3, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  ...                                context_char_tokens\n",
       "26862           What was replaced by the spinning wheel?  ...  [[6, 7], [5, 19, 10, 6, 14, 15, 12, 4, 15, 10,...\n",
       "91742  Around how many camps were set up by the Germa...  ...  [[9, 11, 8, 10, 4, 12, 21], [5, 17, 4, 3, 10],...\n",
       "15093  Who recognized hydrogen gas as a discreet subs...  ...  [[6, 7], [27, 46, 45, 27], [23], [10, 8, 22, 3...\n",
       "22826    On what date did the Japanese land on Enewetak?  ...  [[6, 7], [27, 33, 27, 44], [23], [34, 5, 18, 5...\n",
       "63456  How many \"voices\" did Montini's posters claim ...  ...  [[13, 15, 10, 6, 7, 19], [11, 6, 9], [18, 3, 1...\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset after tokenization\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9I3608UoZWE"
   },
   "source": [
    "## Computation of Max Word and Character lengths for Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Efy4Zh4MYrY-",
    "outputId": "670378f1-f7a8-4cb1-f62e-871046307ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in question:  32\n",
      "Mean number of words in question:  11.133196436566877\n"
     ]
    }
   ],
   "source": [
    "question_word_token_lens = []\n",
    "\n",
    "for i in df_train['question_word_tokens'].values:\n",
    "    question_word_token_lens.extend([len(i)])\n",
    "    \n",
    "print(\"Max number of words in question: \",np.array(question_word_token_lens).max())\n",
    "print(\"Mean number of words in question: \",np.array(question_word_token_lens).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tnrVc9rPYrZA",
    "outputId": "0eed8d91-5af0-4a64-f06f-4dc3a5006c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in context:  340\n",
      "Mean number of words in context:  131.99289783692817\n"
     ]
    }
   ],
   "source": [
    "context_word_token_lens = []\n",
    "\n",
    "for i in df_train['context_word_tokens'].values:\n",
    "    context_word_token_lens.extend([len(i)])\n",
    "    \n",
    "print(\"Max number of words in context: \",np.array(context_word_token_lens).max())\n",
    "print(\"Mean number of words in context: \",np.array(context_word_token_lens).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRMGFKjdbT-b"
   },
   "outputs": [],
   "source": [
    "question_max = np.array(question_word_token_lens).max()\n",
    "context_max = np.array(context_word_token_lens).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "n0S6Ck7_oZWK",
    "outputId": "16658d48-f85b-4b9b-d921-ae0365a8c403"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64769it [00:18, 3574.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of word:  37\n",
      "Mean length of word:  1.3062994894198647\n",
      "99.9 percentage of words have a character length of :  16.0\n"
     ]
    }
   ],
   "source": [
    "char_max = 0\n",
    "total_len = 0\n",
    "total_words = 0\n",
    "dist = []\n",
    "\n",
    "for i, row in tqdm(df_train.iterrows()):\n",
    "    for j in row['question_char_tokens']:\n",
    "        dist.append(len(j))\n",
    "        total_len += len(j)\n",
    "        total_words += 1\n",
    "        if len(j)>char_max:\n",
    "            char_max = len(j)\n",
    "            \n",
    "            \n",
    "    for k in row['context_char_tokens']:\n",
    "        dist.append(len(k))\n",
    "        total_len += len(j)\n",
    "        total_words += 1\n",
    "        if len(k)>char_max:\n",
    "            char_max = len(k)\n",
    "\n",
    "print(\"Maximum length of word: \", char_max)\n",
    "print(\"Mean length of word: \", total_len/total_words)\n",
    "print(\"99.9 percentage of words have a character length of : \", np.percentile(dist,99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rb6g3qVqIgDs"
   },
   "outputs": [],
   "source": [
    "char_max = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "i77qv6EfJVVb",
    "outputId": "118340ca-c330-41f9-f4e7-f3c43d4a2d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109302\n",
      "1218\n"
     ]
    }
   ],
   "source": [
    "#Number of unique words and characters in the train dataset\n",
    "print(len(word_tokenizer))\n",
    "print(len(char_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eCrsiYHoZWL"
   },
   "source": [
    "## Padding Word Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCl4vWlUbT-d"
   },
   "outputs": [],
   "source": [
    "def pad_word_sequences(row):\n",
    "    \"\"\"Function that does padding for word tokens\"\"\"\n",
    "    \n",
    "    question = row['question_word_tokens'].copy()\n",
    "    for i in range(len(question), question_max):\n",
    "        question.append(word_tokenizer['PAD'])\n",
    "        \n",
    "    context = row['context_word_tokens'].copy()\n",
    "    for i in range(len(context), context_max):\n",
    "        context.append(word_tokenizer['PAD'])\n",
    "        \n",
    "    question = np.array(question[:question_max], dtype=np.int32)\n",
    "    context = np.array(context[:context_max], dtype=np.int32)\n",
    "    \n",
    "    row['question_word_padded'] = question\n",
    "    row['context_word_padded'] = context\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n4KNrY7vbT-e",
    "outputId": "4ce1d8a7-47b9-4402-af8b-f41722ed359e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64769/64769 [01:59<00:00, 541.27it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.progress_apply(pad_word_sequences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WQ5h3O-DbT-h",
    "outputId": "dcc1ec26-1871-4ddc-e80a-ec3fc93e5515"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16193/16193 [00:29<00:00, 543.37it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.progress_apply(pad_word_sequences, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgi5JqJ6oZWT"
   },
   "source": [
    "## Padding Character Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcwGwmDDoZWT"
   },
   "outputs": [],
   "source": [
    "def pad_char_sequences(row):\n",
    "    \"\"\"Function that does padding for character tokens\"\"\"\n",
    "    \n",
    "    question = deepcopy(row['question_char_tokens'])\n",
    "    question_chars = []\n",
    "    for i in question:\n",
    "        for j in range(len(i), char_max):\n",
    "            i.append(0)\n",
    "        question_chars.append(np.array(i[:char_max], dtype=np.int32))\n",
    "    \n",
    "    for i in range(len(question_chars), question_max):\n",
    "        question_chars.append(np.zeros(char_max, dtype=np.int32))\n",
    "        \n",
    "    context = deepcopy(row['context_char_tokens'])\n",
    "    context_chars = []\n",
    "    for i in context:\n",
    "        for j in range(len(i), char_max):\n",
    "            i.append(0)\n",
    "        context_chars.append(np.array(i[:char_max], dtype=np.int32))\n",
    "        \n",
    "    for i in range(len(context_chars), context_max):\n",
    "        context_chars.append(np.zeros(char_max, dtype=np.int32))\n",
    "        \n",
    "    question_chars = np.array(question_chars, dtype=np.int32)\n",
    "    context_chars = np.array(context_chars, dtype=np.int32)\n",
    "    \n",
    "    row['question_char_padded'] = question_chars\n",
    "    row['context_char_padded'] = context_chars\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "93d6XImooZWV",
    "outputId": "a0dfa4b8-70d0-413a-f470-595b13bf97d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64769/64769 [03:26<00:00, 314.23it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.progress_apply(pad_char_sequences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o_VJYLuXoZWW",
    "outputId": "2c57a84d-7bb9-412e-fdbe-f7827f078b2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16193/16193 [00:51<00:00, 312.06it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.progress_apply(pad_char_sequences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "0a5mDVNmoZWY",
    "outputId": "d86889ea-3cba-459b-d995-537ba24e5bbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>start_word</th>\n",
       "      <th>end_word</th>\n",
       "      <th>question_word_tokens</th>\n",
       "      <th>context_word_tokens</th>\n",
       "      <th>question_char_tokens</th>\n",
       "      <th>context_char_tokens</th>\n",
       "      <th>question_word_padded</th>\n",
       "      <th>context_word_padded</th>\n",
       "      <th>question_char_padded</th>\n",
       "      <th>context_char_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87248</th>\n",
       "      <td>Who is the second largest business district em...</td>\n",
       "      <td>La Défense</td>\n",
       "      <td>The second-largest business district in terms ...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
       "      <td>[11, 12, 7, 8, 13, 14, 15, 16, 3, 17, 18, 19, ...</td>\n",
       "      <td>[[20, 11, 8], [6, 9], [4, 11, 3], [9, 3, 14, 8...</td>\n",
       "      <td>[[4, 11, 3], [9, 3, 14, 8, 7, 13, 31, 12, 5, 1...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[11, 12, 7, 8, 13, 14, 15, 16, 3, 17, 18, 19, ...</td>\n",
       "      <td>[[20, 11, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[4, 11, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49617</th>\n",
       "      <td>Which language tree groups Dutch with English?</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Within the Indo-European language tree, Dutch ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[73, 74, 75, 76, 77, 78, 79, 10]</td>\n",
       "      <td>[80, 4, 81, 74, 75, 19, 77, 3, 82, 83, 4, 84, ...</td>\n",
       "      <td>[[20, 11, 6, 14, 11], [12, 5, 7, 19, 15, 5, 19...</td>\n",
       "      <td>[[20, 6, 4, 11, 6, 7], [4, 11, 3], [6, 7, 13, ...</td>\n",
       "      <td>[73, 74, 75, 76, 77, 78, 79, 10, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[80, 4, 81, 74, 75, 19, 77, 3, 82, 83, 4, 84, ...</td>\n",
       "      <td>[[20, 11, 6, 14, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[20, 6, 4, 11, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27058</th>\n",
       "      <td>What is the act of analyzing morphophones called?</td>\n",
       "      <td>morphophonology</td>\n",
       "      <td>Since the early 1960s, theoretical linguists h...</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>[124, 3, 4, 125, 15, 126, 127, 128, 10]</td>\n",
       "      <td>[129, 4, 130, 131, 19, 132, 133, 115, 134, 135...</td>\n",
       "      <td>[[20, 11, 5, 4], [6, 9], [4, 11, 3], [5, 14, 4...</td>\n",
       "      <td>[[9, 6, 7, 14, 3], [4, 11, 3], [3, 5, 10, 12, ...</td>\n",
       "      <td>[124, 3, 4, 125, 15, 126, 127, 128, 10, 0, 0, ...</td>\n",
       "      <td>[129, 4, 130, 131, 19, 132, 133, 115, 134, 135...</td>\n",
       "      <td>[[20, 11, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[9, 6, 7, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117665</th>\n",
       "      <td>Who is TrainOSE a subsidiary of?</td>\n",
       "      <td>the Hellenic Railways Organization</td>\n",
       "      <td>Greece's rail network is estimated to be at 2,...</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>[2, 3, 158, 66, 159, 15, 10]</td>\n",
       "      <td>[160, 103, 161, 162, 3, 163, 99, 151, 142, 164...</td>\n",
       "      <td>[[20, 11, 8], [6, 9], [4, 10, 5, 6, 7, 8, 9, 3...</td>\n",
       "      <td>[[19, 10, 3, 3, 14, 3], [37, 9], [10, 5, 6, 12...</td>\n",
       "      <td>[2, 3, 158, 66, 159, 15, 10, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[160, 103, 161, 162, 3, 163, 99, 151, 142, 164...</td>\n",
       "      <td>[[20, 11, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[19, 10, 3, 3, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16521</th>\n",
       "      <td>What is a first person singular  feature of th...</td>\n",
       "      <td>non-ending</td>\n",
       "      <td>In the Balearic Islands, IEC's standard is use...</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>[124, 3, 66, 210, 211, 212, 213, 15, 4, 214, 1...</td>\n",
       "      <td>[31, 4, 214, 215, 19, 216, 103, 177, 3, 217, 2...</td>\n",
       "      <td>[[20, 11, 5, 4], [6, 9], [5], [17, 6, 10, 9, 4...</td>\n",
       "      <td>[[6, 7], [4, 11, 3], [22, 5, 12, 3, 5, 10, 6, ...</td>\n",
       "      <td>[124, 3, 66, 210, 211, 212, 213, 15, 4, 214, 1...</td>\n",
       "      <td>[31, 4, 214, 215, 19, 216, 103, 177, 3, 217, 2...</td>\n",
       "      <td>[[20, 11, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  ...                                context_char_padded\n",
       "87248   Who is the second largest business district em...  ...  [[4, 11, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "49617      Which language tree groups Dutch with English?  ...  [[20, 6, 4, 11, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "27058   What is the act of analyzing morphophones called?  ...  [[9, 6, 7, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "117665                   Who is TrainOSE a subsidiary of?  ...  [[19, 10, 3, 3, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "16521   What is a first person singular  feature of th...  ...  [[6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset after padding\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tp9g4EBSoZWb"
   },
   "source": [
    "## Copying the padded sequences to input data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arsSJp1lbT_O"
   },
   "outputs": [],
   "source": [
    "#creating train input arrays\n",
    "train_context_word_padded = np.asarray(df_train['context_word_padded'].values.tolist(), dtype=np.int32)\n",
    "train_question_word_padded = np.asarray(df_train['question_word_padded'].values.tolist(), dtype=np.int32)\n",
    "test_context_word_padded = np.asarray(df_test['context_word_padded'].values.tolist(), dtype=np.int32)\n",
    "test_question_word_padded = np.asarray(df_test['question_word_padded'].values.tolist(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSjuSOesoZWd"
   },
   "outputs": [],
   "source": [
    "#creating test input arrays\n",
    "train_context_char_padded = np.asarray(df_train['context_char_padded'].values.tolist())\n",
    "train_question_char_padded = np.asarray(df_train['question_char_padded'].values.tolist())\n",
    "test_context_char_padded = np.asarray(df_test['context_char_padded'].values.tolist())\n",
    "test_question_char_padded = np.asarray(df_test['question_char_padded'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBRHoFxioZWf"
   },
   "source": [
    "## Creating Output Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlDZXWbPYrZF"
   },
   "outputs": [],
   "source": [
    "#creating output arrays\n",
    "num_classes = context_max\n",
    "y_start_train = keras.utils.to_categorical(df_train['start_word'].values, num_classes)\n",
    "y_end_train = keras.utils.to_categorical(df_train['end_word'].values, num_classes)\n",
    "\n",
    "y_start_test = keras.utils.to_categorical(df_test['start_word'].values, num_classes)\n",
    "y_end_test = keras.utils.to_categorical(df_test['end_word'].values, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxOWUDrChS5b"
   },
   "source": [
    "## Saving all the required variables to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzN324t9Yiav"
   },
   "outputs": [],
   "source": [
    "#saving word tokenizer\n",
    "with open('word_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "%cp word_tokenizer.pickle \"drive/My Drive/Colab Notebooks/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkCy3EANl--J"
   },
   "outputs": [],
   "source": [
    "#saving character tokenizer\n",
    "with open('char_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(char_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "%cp char_tokenizer.pickle \"drive/My Drive/Colab Notebooks/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqUhrGC9eqHj"
   },
   "outputs": [],
   "source": [
    "#saving train input data\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_context_word_padded.npy\", train_context_word_padded)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_question_word_padded.npy\", train_question_word_padded)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_context_char_padded.npy\", train_context_char_padded)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_question_char_padded.npy\", train_question_char_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iacMqmh4fRjk"
   },
   "outputs": [],
   "source": [
    "#saving test input data\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_context_word_padded.npy\", test_context_word_padded)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_question_word_padded.npy\", test_question_word_padded)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_context_char_padded.npy\", test_context_char_padded)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_question_char_padded.npy\", test_question_char_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiWIeMlViB8B"
   },
   "outputs": [],
   "source": [
    "#saving output data\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/y_start_train.npy\", y_start_train)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/y_end_train.npy\", y_end_train)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/y_start_test.npy\", y_start_test)\n",
    "save(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/y_end_test.npy\", y_end_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SQuAD_SequenceModel_forloop_2.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
