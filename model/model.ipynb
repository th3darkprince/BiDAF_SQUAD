{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJzuqd7PYrYY"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "pLFKz22kYrYZ",
    "outputId": "69b34f82-9085-4bb2-ef07-48b8dcec56f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, GRU, Dense, Embedding, Bidirectional, TimeDistributed   #layers required for network\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Softmax, Concatenate ,Dropout, MaxPool1D        #layers required for network\n",
    "from tensorflow.keras.backend import expand_dims, tile, concatenate, shape, batch_dot, squeeze     #functions required for network\n",
    "import tensorflow.keras.backend as K                                                               #to build metric\n",
    "from tensorflow.keras.models import Model                                                          #to build model\n",
    "from tensorflow.keras.callbacks import TensorBoard                                                 #tensorboard\n",
    "import tensorflow as tf                                                                            #other functions\n",
    "from tqdm import tqdm                                                                              #track progress\n",
    "import numpy as np                                                                                 #for numpy operations\n",
    "import pickle                                                                                      #loading tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FRZJIIOAYrYd",
    "outputId": "e0fc4098-f099-4354-ea9c-99b43c485cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "_GGIAMzAYrYf",
    "outputId": "34d3cc92-e5b9-41ed-a078-dc622d10d37a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8522293018181653761\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2748057066765156713\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11171406046308396574\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15701401920\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6799705826100236329\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "W-gkpaCWf15-",
    "outputId": "368085eb-5d51-4a5c-b1bf-a9267512c0cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "NJQNmeBdiyO9",
    "outputId": "45678223-c595-4150-de87-ed766e1ef5e6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRMGFKjdbT-b"
   },
   "outputs": [],
   "source": [
    "#referred from preproceesing\n",
    "question_max = 32\n",
    "context_max = 340\n",
    "char_max = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axA7pZwshL0R"
   },
   "source": [
    "## Loading all the Required Variables from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-r_RGgBf2u6"
   },
   "outputs": [],
   "source": [
    "#train input arrays\n",
    "train_context_word_padded = load(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_context_word_padded.npy\")\n",
    "train_question_word_padded = load(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_question_word_padded.npy\")\n",
    "train_context_char_padded = load(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_context_char_padded.npy\")\n",
    "train_question_char_padded = load(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/train_question_char_padded.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOuUKlQBgJ95"
   },
   "outputs": [],
   "source": [
    "#test input arrays\n",
    "test_context_word_padded = load(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_context_word_padded.npy\")\n",
    "test_question_word_padded = load(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_question_word_padded.npy\")\n",
    "test_context_char_padded = load(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_context_char_padded.npy\")\n",
    "test_question_char_padded = load(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/test_question_char_padded.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wi9vEAXie_Y"
   },
   "outputs": [],
   "source": [
    "#output arrays\n",
    "y_start_train = load(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/y_start_train.npy\")\n",
    "y_end_train = load(\"drive/My Drive/Colab Notebooks/dataset/train_arrays/y_end_train.npy\")\n",
    "y_start_test = load(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/y_start_test.npy\")\n",
    "y_end_test = load(\"drive/My Drive/Colab Notebooks/dataset/test_arrays/y_end_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yejcaHmZc8Os"
   },
   "outputs": [],
   "source": [
    "#word tokenizer\n",
    "with open('drive/My Drive/Colab Notebooks/dataset/word_tokenizer.pickle', 'rb') as handle:\n",
    " word_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-QDDwZydGM0"
   },
   "outputs": [],
   "source": [
    "#character tokenizer\n",
    "with open('drive/My Drive/Colab Notebooks/dataset/char_tokenizer.pickle', 'rb') as handle:\n",
    " char_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg9ZTUeiYrZG"
   },
   "source": [
    "## Defining Embedding Matrix For Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U7EeiBb-YrZH",
    "outputId": "44c30c85-18eb-41de-8b6f-bdf5a67c2522"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:16, 24229.76it/s]\n"
     ]
    }
   ],
   "source": [
    "enc_data = {}\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/glove.6B.100d.txt\",'rb') as f:\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        enc_data[word.decode('utf-8')] = vector\n",
    "    glove_words = set(enc_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zP3-rxEmYrZI"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "embedding_matrix_word = np.zeros((len(word_tokenizer)+1, 100))\n",
    "for word, i in word_tokenizer.items():\n",
    "    embedding_vector = enc_data.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        count += 1\n",
    "        embedding_matrix_word[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aLVk6C7ebT-q",
    "outputId": "b5b22bff-b7d0-4f52-d9af-ef39bf2978a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words covered by Glove vectors: 40.99284551060365\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of words covered by Glove vectors:\", count/len(word_tokenizer)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDBqlqdPoZWm"
   },
   "source": [
    "## Defining Embedding Matrix For Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qj5CbYDoZWm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix_char = []\n",
    "embedding_matrix_char.append(np.zeros(len(char_tokenizer.word_index)))\n",
    "\n",
    "for char, i in char_tokenizer.word_index.items():\n",
    "    onehot = np.zeros(len(char_tokenizer.word_index))\n",
    "    onehot[i-1] = 1\n",
    "    embedding_matrix_char.append(onehot)\n",
    "\n",
    "embedding_matrix_char = np.array(embedding_matrix_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkghmIC3YrZL"
   },
   "source": [
    "## Defining the Layers of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_b-tfaepYrZL"
   },
   "source": [
    "### Word Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vf_qCbUHYrZL"
   },
   "outputs": [],
   "source": [
    "class word_embedding_layer(Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, input_len):\n",
    "        \n",
    "        super(word_embedding_layer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_len = input_len\n",
    "        self.word_embed = Embedding(self.input_dim, self.output_dim, weights = [embedding_matrix_word], \n",
    "                               input_length = input_len, trainable = False, name = self._name+\"_layer\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        question, context = inputs\n",
    "        return self.word_embed(question), self.word_embed(context) \n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'input_len': self.input_len\n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SW8TaFYoZWp"
   },
   "source": [
    "### Character Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUDwifLloZWq"
   },
   "outputs": [],
   "source": [
    "class char_embedding_layer(Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, input_len):\n",
    "        \n",
    "        super(char_embedding_layer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_len = input_len\n",
    "        self.char_embed = Embedding(self.input_dim, self.output_dim, weights = [embedding_matrix_char], \n",
    "                               input_length = input_len, trainable = False)\n",
    "        self.timed = TimeDistributed(self.char_embed)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        question, context = inputs\n",
    "        return self.timed(question), self.timed(context)\n",
    "            \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'input_len': self.input_len\n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEu-awz5dnB6"
   },
   "outputs": [],
   "source": [
    "class char_cnn_layer(Layer):\n",
    "    \n",
    "    def __init__(self, n_filters, filter_width):\n",
    "        \n",
    "        super(char_cnn_layer, self).__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_width = filter_width\n",
    "        self.conv = Conv1D(self.n_filters, self.filter_width)\n",
    "        self.timed = TimeDistributed(self.conv)\n",
    "          \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        question, context = inputs\n",
    "        return tf.math.reduce_max(self.timed(question), 2), tf.math.reduce_max(self.timed(context), 2)\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_filters': self.n_filters,\n",
    "            'filter_width': self.filter_width\n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkbPCZisoZWs"
   },
   "source": [
    "### Highway Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fJjp8bIoZWs"
   },
   "outputs": [],
   "source": [
    "class highway_input_layer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(highway_input_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        q_w, c_w, q_c, c_c = inputs\n",
    "        question = concatenate([q_w, q_c], axis=2)  \n",
    "        context = concatenate([c_w, c_c], axis=2)  \n",
    "        \n",
    "        return context, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4I-cnUl3oZWu"
   },
   "outputs": [],
   "source": [
    "class highway_layer(Layer):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \n",
    "        super(highway_layer, self).__init__()\n",
    "        self._name = name\n",
    "        self.normal = Dense(200, activation = \"relu\")\n",
    "        self.gate = Dense(200, activation = \"sigmoid\")\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        \n",
    "        n = self.normal(inputs)\n",
    "        g = self.gate(inputs)\n",
    "        x = g*n + (1-g)*inputs\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'name': self._name\n",
    "            \n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpvY4EAmYrZN"
   },
   "source": [
    "### Contextual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G1vtqQ5YrZN"
   },
   "outputs": [],
   "source": [
    "class contextual_layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, name):\n",
    "        \n",
    "        super(contextual_layer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self._name = name       \n",
    "        self.contextual = Bidirectional(GRU(self.output_dim, return_sequences=True, dropout=0.2, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=67)))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True \n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.contextual(inputs)\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "            'name': self._name\n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8QBnIHk9YrZP"
   },
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RerAm-oYrZR"
   },
   "outputs": [],
   "source": [
    "class attention_input_layer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(attention_input_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        H,U = inputs\n",
    "        \n",
    "        expand_h = concatenate([[1,1],[shape(U)[1]],[1]],0)\n",
    "        expand_u = concatenate([[1],[shape(H)[1]],[1,1]],0)\n",
    "    \n",
    "        h = tile(expand_dims(H, axis=2), expand_h)\n",
    "        u = tile(expand_dims(U, axis=1), expand_u)\n",
    "        h_u = h * u\n",
    "        \n",
    "        return concatenate([h,u,h_u], axis=-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Yb9zqi1YrZQ"
   },
   "outputs": [],
   "source": [
    "class attention_layer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(attention_layer, self).__init__()\n",
    "        self.dense = Dense(1, activation = \"linear\", kernel_initializer=tf.keras.initializers.glorot_uniform(seed=54))\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        sim_matrix = self.dense(inputs)\n",
    "        sim_matrix = squeeze(sim_matrix, 3)\n",
    "        \n",
    "        return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GI08Ke5GYrZT"
   },
   "outputs": [],
   "source": [
    "class c2q_q2c_layer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(c2q_q2c_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        sim_matrix, H, U = inputs\n",
    "    \n",
    "        c2q = batch_dot(tf.nn.softmax(sim_matrix, -1), U)\n",
    "        \n",
    "        q2c = batch_dot(tf.nn.softmax(tf.math.reduce_max(sim_matrix, 2), -1), H)\n",
    "        q2c = tile(expand_dims(q2c, axis=1),[1,shape(H)[1],1])\n",
    "        \n",
    "        return c2q, q2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkcqIOqxYrZU"
   },
   "source": [
    "### Modelling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYglE4kmYrZV"
   },
   "outputs": [],
   "source": [
    "class modelling_input_layer(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(modelling_input_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        H, c2q, q2c = inputs\n",
    "        G = concatenate([H, c2q, (H*c2q), (H*q2c)], axis=2)\n",
    "        \n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrBv61sjYrZW"
   },
   "outputs": [],
   "source": [
    "class modelling_layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim):\n",
    "        \n",
    "        super(modelling_layer, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.modelling1 = Bidirectional(GRU(self.output_dim, return_sequences=True, dropout=0.2))\n",
    "        self.modelling2 = Bidirectional(GRU(self.output_dim, return_sequences=True, dropout=0.2))\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.modelling2(self.modelling1(inputs))\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuTXgszVYrZY"
   },
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aO_mKCeYrZY"
   },
   "outputs": [],
   "source": [
    "class input_to_start(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(input_to_start, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        G, M = inputs\n",
    "        GM = concatenate([G, M], axis=2)\n",
    "        \n",
    "        return GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJfh8XQsYrZa"
   },
   "outputs": [],
   "source": [
    "class output_start(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(output_start, self).__init__()\n",
    "        self.dense = Dense(1, activation = \"linear\", kernel_initializer=tf.keras.initializers.glorot_uniform(seed=35))\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        GM = inputs\n",
    "        start = self.dense(GM)\n",
    "        start = self.dropout(start)\n",
    "        p1 = tf.nn.softmax(squeeze(start, axis=2))\n",
    "        \n",
    "        return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DiNgCf2YrZb"
   },
   "outputs": [],
   "source": [
    "class input_to_end(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim):\n",
    "        \n",
    "        super(input_to_end, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.end = Bidirectional(GRU(self.output_dim, return_sequences=True, dropout=0.2, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)))\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        G, M = inputs\n",
    "        M2 = self.end(M)\n",
    "        GM2 = concatenate([G, M2], axis=2)\n",
    "        return GM2\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xY4PmUS3YrZd"
   },
   "outputs": [],
   "source": [
    "class output_end(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(output_end, self).__init__()\n",
    "        self.dense = Dense(1, activation = \"linear\", kernel_initializer=tf.keras.initializers.glorot_uniform(seed=85))\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        GM2 = inputs\n",
    "        end = self.dense(GM2)\n",
    "        end = self.dropout(end)\n",
    "        p2 = tf.nn.softmax(squeeze(end, axis=2))\n",
    "        \n",
    "        return p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4N1JgAKYrZe"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHfjyz_ZYrZf"
   },
   "outputs": [],
   "source": [
    "#https://medium.com/@aakashgoel12/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n",
    "def f1_score(y_true, y_pred):    #taken from old keras source code\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3peeA-8YrZg"
   },
   "outputs": [],
   "source": [
    "def bidaf_model(question_timesteps, context_timesteps, hidden_size, char_vocab, n_filters, filter_width):\n",
    "    \"\"\"Function that build the BiDAF model using the custom class layers\"\"\"\n",
    "    \n",
    "    #inputs\n",
    "    question_words = Input(shape=(question_timesteps,), name = 'question_word_tokens')\n",
    "    context_words = Input(shape=(context_timesteps,), name = 'context_word_tokens')    \n",
    "    question_chars = Input(shape=(question_timesteps,char_max,), name = 'question_char_tokens')\n",
    "    context_chars = Input(shape=(context_timesteps,char_max,), name = 'context_char_tokens')\n",
    "\n",
    "    #word embedding layer\n",
    "    question_word_embedded, context_word_embedded = word_embedding_layer(len(word_tokenizer)+1, hidden_size, question_words.shape[-1])([question_words, context_words])\n",
    "            \n",
    "    #character embedding layer\n",
    "    question_char_embedded, context_char_embedded = char_embedding_layer(len(char_tokenizer.word_index)+1, char_vocab, question_chars.shape[-1])([question_chars, context_chars])\n",
    "    \n",
    "    #character CNN\n",
    "    question_char_embedded, context_char_embedded  = char_cnn_layer(n_filters, filter_width)([question_char_embedded, context_char_embedded])\n",
    "    \n",
    "    #highway layer\n",
    "    context, question = highway_input_layer()([question_word_embedded, context_word_embedded, question_char_embedded, context_char_embedded])\n",
    "    question_blendrep = highway_layer(\"question_highway\")(question)\n",
    "    context_blendrep = highway_layer(\"context_highway\")(context)\n",
    "\n",
    "    #contextual layer\n",
    "    question_contextual = contextual_layer(hidden_size, \"question_contextual\")(question_blendrep)\n",
    "    context_contextual = contextual_layer(hidden_size, \"context_contextual\")(context_blendrep)\n",
    "\n",
    "    #attention layer\n",
    "    attention_in = attention_input_layer()([context_contextual, question_contextual])\n",
    "    attention_out = attention_layer()(attention_in)\n",
    "    c2q, q2c = c2q_q2c_layer()([attention_out, context_contextual, question_contextual])\n",
    "    \n",
    "    #modelling layer\n",
    "    G = modelling_input_layer()([context_contextual, c2q, q2c])\n",
    "    M = modelling_layer(hidden_size)(G)\n",
    "    \n",
    "    #output layers\n",
    "    GM = input_to_start()([G,M])\n",
    "    start = output_start()(GM)\n",
    "    GM2 = input_to_end(hidden_size)([G,M])\n",
    "    end = output_end()(GM2)\n",
    "\n",
    "    model = Model(inputs=[question_words,context_words,question_chars,context_chars], outputs=[start, end], name=\"bidaf\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBknca-MYrZk"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YjnZWESDYrZw",
    "outputId": "5d0dee74-6473-44b7-8994-e8da2c71ecbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bidaf\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "question_char_tokens (InputLaye [(None, 32, 15)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context_char_tokens (InputLayer [(None, 340, 15)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_word_tokens (InputLaye [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context_word_tokens (InputLayer [(None, 340)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding_layer (char_embe ((None, 32, 15, 1218 1484742     question_char_tokens[0][0]       \n",
      "                                                                 context_char_tokens[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (word_embe ((None, 32, 100), (N 10930300    question_word_tokens[0][0]       \n",
      "                                                                 context_word_tokens[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "char_cnn_layer (char_cnn_layer) ((None, 32, 100), (N 365500      char_embedding_layer[0][0]       \n",
      "                                                                 char_embedding_layer[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "highway_input_layer (highway_in ((None, 340, 200), ( 0           word_embedding_layer[0][0]       \n",
      "                                                                 word_embedding_layer[0][1]       \n",
      "                                                                 char_cnn_layer[0][0]             \n",
      "                                                                 char_cnn_layer[0][1]             \n",
      "__________________________________________________________________________________________________\n",
      "context_highway (highway_layer) (None, 340, 200)     80400       highway_input_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "question_highway (highway_layer (None, 32, 200)      80400       highway_input_layer[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "context_contextual (contextual_ (None, 340, 200)     181200      context_highway[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "question_contextual (contextual (None, 32, 200)      181200      question_highway[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_input_layer (attentio (None, 340, 32, 600) 0           context_contextual[0][0]         \n",
      "                                                                 question_contextual[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (attention_laye (None, 340, 32)      601         attention_input_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "c2q_q2c_layer (c2q_q2c_layer)   ((None, 340, 200), ( 0           attention_layer[0][0]            \n",
      "                                                                 context_contextual[0][0]         \n",
      "                                                                 question_contextual[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "modelling_input_layer (modellin (None, 340, 800)     0           context_contextual[0][0]         \n",
      "                                                                 c2q_q2c_layer[0][0]              \n",
      "                                                                 c2q_q2c_layer[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "modelling_layer (modelling_laye (None, 340, 200)     722400      modelling_input_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_to_start (input_to_start) (None, 340, 1000)    0           modelling_input_layer[0][0]      \n",
      "                                                                 modelling_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_to_end (input_to_end)     (None, 340, 1000)    181200      modelling_input_layer[0][0]      \n",
      "                                                                 modelling_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output_start (output_start)     (None, 340)          1001        input_to_start[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output_end (output_end)         (None, 340)          1001        input_to_end[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 14,209,945\n",
      "Trainable params: 1,794,903\n",
      "Non-trainable params: 12,415,042\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#defining various parameters of the model and building it\n",
    "\n",
    "#input timesteps\n",
    "question_timesteps = question_max\n",
    "context_timesteps = context_max\n",
    "\n",
    "#output dimensions of word and character embedding\n",
    "hidden_size = 100\n",
    "char_vocab = 1218\n",
    "\n",
    "#character CNN filters and width\n",
    "n_filters = 100\n",
    "filter_width = 3\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = bidaf_model(question_timesteps, context_timesteps, hidden_size, char_vocab, n_filters, filter_width)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRtdj4z8jc2I"
   },
   "outputs": [],
   "source": [
    "#defining loss function and optimizer\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy(reduction='auto')\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ne9wT7ZYrFPu"
   },
   "source": [
    "## Data Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MluMH2ljcYz"
   },
   "source": [
    "### Prepare Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmCXqUEBYrZx"
   },
   "outputs": [],
   "source": [
    "##creating input dataset(train and test) using tf.data\n",
    "train_inputs = tf.data.Dataset.from_tensor_slices((train_question_word_padded, train_context_word_padded, train_question_char_padded, train_context_char_padded))\n",
    "test_inputs = tf.data.Dataset.from_tensor_slices((test_question_word_padded, test_context_word_padded, test_question_char_padded, test_context_char_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8DdhiFvYrZy"
   },
   "outputs": [],
   "source": [
    "##creating output dataset(train and test) using tf.data\n",
    "train_targets = tf.data.Dataset.from_tensor_slices((y_start_train, y_end_train))\n",
    "test_targets = tf.data.Dataset.from_tensor_slices((y_start_test, y_end_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-RgRwG2YrZ0"
   },
   "outputs": [],
   "source": [
    "#shuffling and split to batches\n",
    "train_dataset = tf.data.Dataset.zip((train_inputs, train_targets)).shuffle(500).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.zip((test_inputs, test_targets)).shuffle(500).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzL1UgL7jmLc"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9Njoz3PYrZ6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn\n",
    "#steps to be performed in each training step\n",
    "@tf.function\n",
    "def train_step(input_vector, output_vector,loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward propagation\n",
    "        output_predicted = model(inputs=input_vector, training=True)\n",
    "        #loss\n",
    "        loss_start = loss_function(output_vector[0], output_predicted[0])\n",
    "        loss_end = loss_function(output_vector[1], output_predicted[1])\n",
    "        loss_final = loss_start + loss_end\n",
    "    #getting gradients\n",
    "    gradients = tape.gradient(loss_final, model.trainable_variables)\n",
    "    #applying gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss_start, loss_end, output_predicted, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Tgpvs5_jc2S"
   },
   "outputs": [],
   "source": [
    "#https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn\n",
    "#steps to be performed in each validation step\n",
    "@tf.function\n",
    "def val_step(input_vector, output_vector, loss_fn):\n",
    "    #getting output of validation data\n",
    "    output_predicted = model(inputs=input_vector, training=False)\n",
    "    #loss calculation\n",
    "    loss_start = loss_function(output_vector[0], output_predicted[0])\n",
    "    loss_end = loss_function(output_vector[1], output_predicted[1])\n",
    "    return loss_start, loss_end, output_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whu-bf8Ujc2T"
   },
   "outputs": [],
   "source": [
    "#batch size\n",
    "BATCH_SIZE=32\n",
    "##number of epochs\n",
    "EPOCHS=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4HATh0Fjc2V"
   },
   "outputs": [],
   "source": [
    "#defining functions to compute the mean loss for each epoch\n",
    "train_start_loss = tf.keras.metrics.Mean(name='train_start_loss')\n",
    "train_end_loss = tf.keras.metrics.Mean(name='train_end_loss')\n",
    "val_start_loss = tf.keras.metrics.Mean(name='val_start_loss')\n",
    "val_end_loss = tf.keras.metrics.Mean(name='val_end_loss')\n",
    "train_start_f1 = tf.keras.metrics.Mean(name=\"train_start_f1\")\n",
    "train_end_f1 = tf.keras.metrics.Mean(name=\"train_end_f1\")\n",
    "val_start_f1 = tf.keras.metrics.Mean(name=\"val_start_f1\")\n",
    "val_end_f1 = tf.keras.metrics.Mean(name=\"val_end_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MDVggVcjc2X"
   },
   "outputs": [],
   "source": [
    "#tensorboard file writers\n",
    "wtrain = tf.summary.create_file_writer(logdir='logdir_train')\n",
    "wval = tf.summary.create_file_writer(logdir='logdir_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJN9naSkjc2Y"
   },
   "outputs": [],
   "source": [
    "#to get the iteration number for recording logs\n",
    "iters = math.ceil(64769/32) \n",
    "#for model checkpointing\n",
    "best_loss=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "colab_type": "code",
    "id": "C3CsVhntjc2a",
    "outputId": "f8fcb963-a331-40ba-d7e5-d36092c5f6b4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:57<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Start Loss: 3.943047, Start F1 Score: 0.04568, Train End Loss: 3.737789, End F1 Score: 0.05230,\n",
      "    Val Start Loss: 3.040010, Val Start F1 Score: 0.08044, Val End Loss: 2.798793, Val End F1 Score: 0.10350\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:56<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Start Loss: 3.390445, Start F1 Score: 0.12948, Train End Loss: 3.202414, End F1 Score: 0.14917,\n",
      "    Val Start Loss: 3.023395, Val Start F1 Score: 0.11656, Val End Loss: 2.646866, Val End F1 Score: 0.14655\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [24:47<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Start Loss: 2.928725, Start F1 Score: 0.28020, Train End Loss: 2.746392, End F1 Score: 0.31691,\n",
      "    Val Start Loss: 2.024346, Val Start F1 Score: 0.36717, Val End Loss: 1.816877, Val End F1 Score: 0.44213\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [24:15<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Start Loss: 2.641253, Start F1 Score: 0.37690, Train End Loss: 2.476184, End F1 Score: 0.41913,\n",
      "    Val Start Loss: 1.830245, Val Start F1 Score: 0.44721, Val End Loss: 1.644780, Val End F1 Score: 0.50447\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:56<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Start Loss: 2.522749, Start F1 Score: 0.41432, Train End Loss: 2.356189, End F1 Score: 0.45976,\n",
      "    Val Start Loss: 1.769097, Val Start F1 Score: 0.46996, Val End Loss: 1.606666, Val End F1 Score: 0.51616\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:51<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Start Loss: 2.408644, Start F1 Score: 0.44738, Train End Loss: 2.250348, End F1 Score: 0.49462,\n",
      "    Val Start Loss: 1.726372, Val Start F1 Score: 0.48292, Val End Loss: 1.540575, Val End F1 Score: 0.54831\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:50<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Start Loss: 2.327476, Start F1 Score: 0.46988, Train End Loss: 2.166238, End F1 Score: 0.51891,\n",
      "    Val Start Loss: 1.697856, Val Start F1 Score: 0.51075, Val End Loss: 1.529177, Val End F1 Score: 0.56338\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:42<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Start Loss: 2.235740, Start F1 Score: 0.49687, Train End Loss: 2.101116, End F1 Score: 0.54083,\n",
      "    Val Start Loss: 1.670072, Val Start F1 Score: 0.51971, Val End Loss: 1.508179, Val End F1 Score: 0.57820\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:51<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Start Loss: 2.175614, Start F1 Score: 0.51717, Train End Loss: 2.046705, End F1 Score: 0.55964,\n",
      "    Val Start Loss: 1.651155, Val Start F1 Score: 0.52256, Val End Loss: 1.483043, Val End F1 Score: 0.57675\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:52<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Start Loss: 2.142618, Start F1 Score: 0.52822, Train End Loss: 1.984587, End F1 Score: 0.57578,\n",
      "    Val Start Loss: 1.651103, Val Start F1 Score: 0.53271, Val End Loss: 1.479795, Val End F1 Score: 0.58703\n",
      "INFO:tensorflow:Assets written to: drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:47<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Start Loss: 2.079690, Start F1 Score: 0.54090, Train End Loss: 1.938480, End F1 Score: 0.59007,\n",
      "    Val Start Loss: 1.658940, Val Start F1 Score: 0.54067, Val End Loss: 1.515988, Val End F1 Score: 0.59370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [23:41<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Start Loss: 2.042344, Start F1 Score: 0.55424, Train End Loss: 1.890511, End F1 Score: 0.60424,\n",
      "    Val Start Loss: 1.652715, Val Start F1 Score: 0.53162, Val End Loss: 1.498028, Val End F1 Score: 0.59067\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    #resetting the states of the loss and metrics\n",
    "    train_start_loss.reset_states()\n",
    "    train_end_loss.reset_states()\n",
    "    val_start_loss.reset_states()\n",
    "    val_end_loss.reset_states()\n",
    "    train_start_f1.reset_states()\n",
    "    train_end_f1.reset_states()\n",
    "    val_start_f1.reset_states()\n",
    "    val_end_f1.reset_states()\n",
    "    \n",
    "    ##counter for train loop iteration\n",
    "    counter = 0\n",
    "    \n",
    "    #ietrating over train data batch by batch\n",
    "    for text_seq, label_seq in tqdm(iterable=train_dataset, total=len(list(train_dataset))):\n",
    "        #train step\n",
    "        loss_start_, loss_end_, pred_out, gradients = train_step(text_seq, label_seq, loss_function)\n",
    "        #adding loss to train loss\n",
    "        train_start_loss(loss_start_)\n",
    "        train_end_loss(loss_end_)\n",
    "        #counting the step number\n",
    "        temp_step = epoch*iters+counter\n",
    "        counter = counter + 1\n",
    "        \n",
    "        #calculating f1 for batch\n",
    "        f1_start = f1_score(label_seq[0], pred_out[0])\n",
    "        f1_end = f1_score(label_seq[1], pred_out[1])\n",
    "        train_start_f1(f1_start)\n",
    "        train_end_f1(f1_end)\n",
    "        \n",
    "        ##tensorboard \n",
    "        with tf.name_scope('per_step_training'):\n",
    "            with wtrain.as_default():\n",
    "                tf.summary.scalar(\"start_loss\", loss_start_, step=temp_step)\n",
    "                tf.summary.scalar(\"end_loss\", loss_end_, step=temp_step)\n",
    "                tf.summary.scalar('f1_start', f1_start, step=temp_step)\n",
    "                tf.summary.scalar('f1_end', f1_end, step=temp_step)\n",
    "        with tf.name_scope(\"per_batch_gradients\"):\n",
    "            with wtrain.as_default():\n",
    "                for i in range(len(model.trainable_variables)):\n",
    "                    name_temp = model.trainable_variables[i].name\n",
    "                    tf.summary.histogram(name_temp, gradients[i], step=temp_step)\n",
    "    \n",
    "    \n",
    "    #validation data\n",
    "    for text_seq_val, label_seq_val in test_dataset:\n",
    "        #getting val output\n",
    "        loss_val_start, loss_val_end, pred_out_val = val_step(text_seq_val, label_seq_val, loss_function)\n",
    "        \n",
    "        val_start_loss(loss_val_start)\n",
    "        val_end_loss(loss_val_end)\n",
    "        \n",
    "        #calculating metric\n",
    "        f1_start_val = f1_score(label_seq_val[0], pred_out_val[0])\n",
    "        f1_end_val = f1_score(label_seq_val[1], pred_out_val[1])\n",
    "        val_start_f1(f1_start_val)\n",
    "        val_end_f1(f1_end_val)\n",
    "    \n",
    "   \n",
    "    #printing\n",
    "    template = '''Epoch {}, Train Start Loss: {:0.6f}, Start F1 Score: {:0.5f}, Train End Loss: {:0.6f}, End F1 Score: {:0.5f},\n",
    "    Val Start Loss: {:0.6f}, Val Start F1 Score: {:0.5f}, Val End Loss: {:0.6f}, Val End F1 Score: {:0.5f}'''\n",
    "\n",
    "    print(template.format(epoch+1, train_start_loss.result(), train_start_f1.result(), \n",
    "                          train_end_loss.result(), train_end_f1.result(),\n",
    "                          val_start_loss.result(), val_start_f1.result(),\n",
    "                          val_end_loss.result(), val_end_f1.result()))\n",
    "\n",
    "\n",
    "    if (val_start_loss.result()+val_end_loss.result())<best_loss:\n",
    "      model.save(\"drive/My Drive/Colab Notebooks/new_model/model\")\n",
    "      best_loss=(val_start_loss.result()+val_end_loss.result())\n",
    "    \n",
    "    #tensorboard\n",
    "    with tf.name_scope(\"per_epoch_loss_metric\"):\n",
    "        with wtrain.as_default():\n",
    "            tf.summary.scalar(\"start_loss\", train_start_loss.result().numpy(), step=epoch)\n",
    "            tf.summary.scalar(\"end_loss\", train_end_loss.result().numpy(), step=epoch)\n",
    "            tf.summary.scalar('start_f1', train_start_f1.result().numpy(), step=epoch)\n",
    "            tf.summary.scalar('end_f1', train_end_f1.result().numpy(), step=epoch)\n",
    "        with wval.as_default():\n",
    "            tf.summary.scalar(\"start_loss\", val_start_loss.result().numpy(), step=epoch)\n",
    "            tf.summary.scalar(\"end_loss\", val_end_loss.result().numpy(), step=epoch)\n",
    "            tf.summary.scalar('start_f1', val_start_f1.result().numpy(), step=epoch)\n",
    "            tf.summary.scalar('end_f1', val_end_f1.result().numpy(), step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLdun-p2yuDd"
   },
   "outputs": [],
   "source": [
    "#saving tensorboard logs\n",
    "%cp logdir_train/*.v2 \"drive/My Drive/Colab Notebooks/logdir/train/\"\n",
    "%cp logdir_val/*.v2 \"drive/My Drive/Colab Notebooks/logdir/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBaIYfx6gBww"
   },
   "outputs": [],
   "source": [
    "def print_predictions(data_point):\n",
    "  \"\"\"Function that takes record numbers as input and predicts the answer for that record\"\"\"\n",
    "\n",
    "  print(\"Question:\")\n",
    "  for i in test_question_word_padded[data_point]:\n",
    "    if i==0:\n",
    "      break\n",
    "    else:\n",
    "      print(list(word_tokenizer.keys())[list(word_tokenizer.values()).index(i)], end = ' ')\n",
    "  print(\"\\nContext:\")\n",
    "  for i in test_context_word_padded[data_point]:\n",
    "    if i==0:\n",
    "      break\n",
    "    else:\n",
    "      print(list(word_tokenizer.keys())[list(word_tokenizer.values()).index(i)], end = ' ')\n",
    "  print(\"\\nPredicted Answer:\")\n",
    "  start, end = model.predict([test_question_word_padded[data_point:data_point+1], test_context_word_padded[data_point:data_point+1], test_question_char_padded[15:16], test_context_char_padded[15:16]])\n",
    "  for i in range(start.argmax(), end.argmax()+1):\n",
    "    print(list(word_tokenizer.keys())[list(word_tokenizer.values()).index(test_context_word_padded[data_point][i])], end=' ')\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "BOVM39K-gl3b",
    "outputId": "a1e17696-2172-400c-e495-f379109353b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What kind of fishing tourism occurs on the island ? \n",
      "Context:\n",
      "The tourist industry is heavily based on the promotion of Napoleon 's imprisonment . A golf course also exists and the possibility for sportfishing tourism is great . Three hotels operate on the island but the arrival of tourists is directly linked to the arrival and departure schedule of the RMS St Helena . Some 3,200 short-term visitors arrived on the island in 2013 . \n",
      "Predicted Answer:\n",
      "sportfishing \n",
      "\n",
      "Question:\n",
      "Who measured the circumference of the Earth ? \n",
      "Context:\n",
      "Hellenistic Geometers such as Archimedes ( c. 287 – 212 BC ) , Apollonius of Perga ( c. 262 – c. 190 BC ) , and Euclid ( c. 325 – 265 BC ) , whose Elements became the most important textbook in mathematics until the 19th century , built upon the work of the Hellenic era Pythagoreans . Euclid developed proofs for the Pythagorean Theorem , for the infinitude of primes , and worked on the ﬁve Platonic solids . Eratosthenes used his knowledge of geometry to measure the circumference of the Earth . His calculation was remarkably accurate . He was also the first to calculate the tilt of the Earth 's axis ( again with remarkable accuracy ) . Additionally , he may have accurately calculated the distance from the Earth to the Sun and invented the leap day . Known as the `` Father of Geography `` , Eratosthenes also created the first map of the world incorporating parallels and meridians , based on the available geographical knowledge of the era . \n",
      "Predicted Answer:\n",
      "Eratosthenes \n",
      "\n",
      "Question:\n",
      "What was the date on the Yeltsin letter ? \n",
      "Context:\n",
      "The Alma-Ata Protocol also addressed other issues , including UN membership . Notably , Russia was authorized to assume the Soviet Union 's UN membership , including its permanent seat on the Security Council . The Soviet Ambassador to the UN delivered a letter signed by Russian President Yeltsin to the UN Secretary-General dated December 24 , 1991 , informing him that by virtue of the Alma-Ata Protocol , Russia was the successor state to the USSR . After being circulated among the other UN member states , with no objection raised , the statement was declared accepted on the last day of the year , December 31 , 1991 . \n",
      "Predicted Answer:\n",
      "December 24 , 1991 \n",
      "\n",
      "Question:\n",
      "What company took over Scottish & Newcastle 's pubs ? \n",
      "Context:\n",
      "In March 2006 , a law was introduced to forbid smoking in all enclosed public places in Scotland . Wales followed suit in April 2007 , with England introducing the ban in July 2007 . Pub landlords had raised concerns prior to the implementation of the law that a smoking ban would have a negative impact on sales . After two years , the impact of the ban was mixed ; some pubs suffered declining sales , while others developed their food sales . The Wetherspoon pub chain reported in June 2009 that profits were at the top end of expectations ; however , Scottish & Newcastle 's takeover by Carlsberg and Heineken was reported in January 2008 as partly the result of its weakness following falling sales due to the ban . Similar bans are applied in Australian pubs with smoking only allowed in designated areas . \n",
      "Predicted Answer:\n",
      "Carlsberg and Heineken \n",
      "\n",
      "Question:\n",
      "How many airlines operate out of Richmond International ? \n",
      "Context:\n",
      "The Greater Richmond area is served by the Richmond International Airport ( IATA : RIC , ICAO : KRIC ) , located in nearby Sandston , seven miles ( 11 km ) southeast of Richmond and within an hour drive of historic Williamsburg , Virginia . Richmond International is now served by nine airlines with over 200 daily flights providing non-stop service to major destination markets and connecting flights to destinations worldwide . A record 3.3 million passengers used Richmond International Airport in 2006 , a 13 % increase over 2005 . \n",
      "Predicted Answer:\n",
      "nine airlines \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_points = [8,15,52,152,332]\n",
    "for i in data_points:\n",
    "  print_predictions(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dpYi3oCOaBvw",
    "outputId": "48a4c703-afdc-466e-9d9c-ee40c9499b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/new_model/model/assets\n"
     ]
    }
   ],
   "source": [
    "#saving the model to disk\n",
    "model.save(\"/content/drive/My Drive/Colab Notebooks/new_model/model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Complete_SQUAD.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
