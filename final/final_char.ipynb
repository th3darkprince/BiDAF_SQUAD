{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\squad\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf                                             #to import the model\n",
    "import pandas as pd                                                 #to import test data\n",
    "import pickle                                                       #to load tokenizers\n",
    "from nltk.tokenize import word_tokenize                             #to tokenize sentences\n",
    "from keras.preprocessing.sequence import pad_sequences              #to pad sequences to max length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbTfQz9iQNx-"
   },
   "source": [
    "## Creating Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bidaf():\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.load_model(\"bidaf/model/\")\n",
    "        with open('bidaf/word_tokenizer.pickle', 'rb') as handle:\n",
    "            self.word_tokenizer = pickle.load(handle)\n",
    "        with open('bidaf/char_tokenizer.pickle', 'rb') as handle:\n",
    "            self.char_tokenizer = pickle.load(handle)\n",
    "            self.context_max = 340\n",
    "            self.question_max = 32\n",
    "            self.char_max = 15\n",
    "            \n",
    "    def __get_tokens(self):\n",
    "        self.question = word_tokenize(self.question)\n",
    "        self.context = word_tokenize(self.context)\n",
    "        \n",
    "        self.question_word_tokens = []\n",
    "        self.context_word_tokens = []\n",
    "        self.question_char_tokens = []\n",
    "        self.context_char_tokens = []\n",
    "        \n",
    "        for i in self.question:\n",
    "            if i in self.word_tokenizer.keys():\n",
    "                self.question_word_tokens.append(self.word_tokenizer[i])\n",
    "                self.question_char_tokens.append(self.char_tokenizer.texts_to_sequences([i])[0])\n",
    "            else:\n",
    "                self.question_word_tokens.append(self.word_tokenizer['UNK'])\n",
    "                self.question_char_tokens.append(self.char_tokenizer.texts_to_sequences([i])[0])\n",
    "                \n",
    "        for i in self.context:\n",
    "            if i in self.word_tokenizer.keys():\n",
    "                self.context_word_tokens.append(self.word_tokenizer[i])\n",
    "                self.context_char_tokens.append(self.char_tokenizer.texts_to_sequences([i])[0])\n",
    "            else:\n",
    "                self.context_word_tokens.append(self.word_tokenizer['UNK'])\n",
    "                self.context_char_tokens.append(self.char_tokenizer.texts_to_sequences([i])[0])\n",
    "                \n",
    "    def __get_padded_word_sequences(self):\n",
    "        for i in range(len(self.question_word_tokens), self.question_max):\n",
    "            self.question_word_tokens.append(self.word_tokenizer['PAD'])\n",
    "\n",
    "        for i in range(len(self.context_word_tokens), self.context_max):\n",
    "            self.context_word_tokens.append(self.word_tokenizer['PAD'])\n",
    "\n",
    "        self.question_word_padded = np.array(self.question_word_tokens[:self.question_max], dtype=np.int32)\n",
    "        self.context_word_padded = np.array(self.context_word_tokens[:self.context_max], dtype=np.int32)\n",
    "        \n",
    "        \n",
    "    def __get_padded_char_sequences(self):\n",
    "        self.question_char_padded = []\n",
    "        for i in self.question_char_tokens:\n",
    "            for j in range(len(i), self.char_max):\n",
    "                i.append(0)\n",
    "            self.question_char_padded.append(np.array(i[:self.char_max], dtype=np.int32))\n",
    "\n",
    "        for i in range(len(self.question_char_padded), self.question_max):\n",
    "            self.question_char_padded.append(np.zeros(self.char_max, dtype=np.int32))\n",
    "\n",
    "        self.context_char_padded = []\n",
    "        for i in self.context_char_tokens:\n",
    "            for j in range(len(i), self.char_max):\n",
    "                i.append(0)\n",
    "            self.context_char_padded.append(np.array(i[:self.char_max], dtype=np.int32))\n",
    "\n",
    "        for i in range(len(self.context_char_padded), self.context_max):\n",
    "            self.context_char_padded.append(np.zeros(self.char_max, dtype=np.int32))\n",
    "\n",
    "        self.question_char_padded = np.array(self.question_char_padded, dtype=np.int32)\n",
    "        self.context_char_padded = np.array(self.context_char_padded, dtype=np.int32)\n",
    "                \n",
    "    def predict(question, context):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.__get_tokens()\n",
    "        self.__get_padded_word_sequences()      \n",
    "        self.__get_padded_char_sequences()\n",
    "        start, end = self.model.predict([self.question_word_padded, self.context_word_padded, \n",
    "                                    elf.question_char_padded, self.context_char_padded])\n",
    "        \n",
    "        for i in range(start.argmax(), end.argmax()+1):\n",
    "            print(self.word_tokenizer[self.context_word_padded[0][i]], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kL5T2zmQNyB"
   },
   "source": [
    "## Loading the Dataset to obtain test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34dVAvVfQNyB"
   },
   "outputs": [],
   "source": [
    "df_answerable = pd.read_pickle(\"bidaf/df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erQPo2ZWRrwG"
   },
   "outputs": [],
   "source": [
    "# input question and context as strings. Here we take them from the dataset\n",
    "question = df_answerable['question'][80184]\n",
    "context = df_answerable['context'][80184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eIVNCXqI3V2-",
    "outputId": "b4e5df77-0689-4863-86a5-d940107d3ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = bidaf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who contributed to the American studies programs at Yale and University of Wyoming?\n",
      "The American studies program reflected the worldwide anti-Communist ideological struggle. Norman Holmes Pearson, who worked for the Office of Strategic Studies in London during World War II, returned to Yale and headed the new American studies program, in which scholarship quickly became an instrument of promoting liberty. Popular among undergraduates, the program sought to instruct them in the fundamentals of American civilization and thereby instill a sense of nationalism and national purpose. Also during the 1940s and 1950s, Wyoming millionaire William Robertson Coe made large contributions to the American studies programs at Yale University and at the University of Wyoming. Coe was concerned to celebrate the 'values' of the Western United States in order to meet the \"threat of communism.\"\n"
     ]
    }
   ],
   "source": [
    "print(question)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(question, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
